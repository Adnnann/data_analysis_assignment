---
title: "AtlantBH task"
output:
  word_document: default
  html_document: default
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(magrittr)
library(ggplot2)
```

```{r master_data, include=FALSE}
data <- read.csv("DataSF_Restaurant_Inspections.csv")
```

# Introduction

Company ZeroPoint has put on the market dataset with locations of
business in San Francisco. The supplier would like to sell POIs data at
the highest market price. To make an informed decision about the
potential procurement of the available dataset, I analyzed the data set
to check data quality and consistency. The total number of data
available in the dataset is 52 315. Data that are considered critical
data (i.e., data of the utmost business importance) are stored in
columns business name, business longitude, and business latitude. Data
analysis was done In R.

## Descriptive statistics

### Missing data

In the first step, I made an analysis of missing values. In total there
are 1 887 missing values in the column business name. Furthermore,
missing data were entered inconsistently, i.e., in some columns data
were entered as "hidden", while in other cases word hidden was entered
with a capital letter. Also, the term Unavailable was used to denote
missing values.

To enable consistent tracking of missing values, all values in column
business name that were entered as either "hidden", "Hidden", or
"Unavailable", and all missing values (NAs) were transformed to "NA"
(string) values. This step was taken to enable consistency in filtering,
grouping, and summarizing data as well as checking the number of missing
data. Analysis revealed that for 1 709 data business name values and 22
674 long. and lat. values are missing in the data set.

### Data accuracy and completeness

To check the correctness of data available in the dataset, firstly I
checked if longitude and latitude values are all between valid ranges
(long. values +-90 degrees and latitude values +-180 degrees). Analysis
revealed that there are 4 data that fall outside valid ranges.

The precision of long. and lat. values are impacted by the number of
decimal points. The relation between the number of decimal points and
the precision of long. and lat. values is presented in the table
below[\[1\]](#_ftn1).

\

------------------------------------------------------------------------

[\[1\]](#_ftnref1) Table was taken from
<https://en.wikipedia.org/wiki/Decimal_degrees>

\

------------------------------------------------------------------------

[\[1\]](#_ftnref1) Table was taken from
<https://en.wikipedia.org/wiki/Decimal_degrees>

+-------+-------+-------+-------+-------+-------+-------+-------+
| De    | De    | DMS   | O     | N/S   | E/W   | E/W   | E/W   |
| cimal | cimal |       | bject | or    | at    | at    | at    |
| p     | de    |       | that  | E/W   | 23N/S | 45N/S | 67N/S |
| laces | grees |       | can   | at eq |       |       |       |
|       |       |       | be un | uator |       |       |       |
|       |       |       | a     |       |       |       |       |
|       |       |       | mbigu |       |       |       |       |
|       |       |       | ously |       |       |       |       |
|       |       |       |       |       |       |       |       |
|       |       |       | recog |       |       |       |       |
|       |       |       | nized |       |       |       |       |
|       |       |       | at    |       |       |       |       |
|       |       |       | this  |       |       |       |       |
|       |       |       | scale |       |       |       |       |
+=======+=======+=======+=======+=======+=======+=======+=======+
| 0     | 1.0   | 1°    | co    | 1     | 1     | 78    | 43    |
|       |       | 00′   | untry | 11 km | 02 km | .7 km | .5 km |
|       |       | 0″    | or    |       |       |       |       |
|       |       |       | large |       |       |       |       |
|       |       |       | r     |       |       |       |       |
|       |       |       | egion |       |       |       |       |
+-------+-------+-------+-------+-------+-------+-------+-------+
| 1     | 0.1   | 0°    | large | 11    | 10    | 7.    | 4.    |
|       |       | 06′   | city  | .1 km | .2 km | 87 km | 35 km |
|       |       | 0″    | or    |       |       |       |       |
|       |       |       | dis   |       |       |       |       |
|       |       |       | trict |       |       |       |       |
+-------+-------+-------+-------+-------+-------+-------+-------+
| 2     | 0.01  | 0°    | town  | 1.    | 1.    | 0.7   | 0.4   |
|       |       | 00′   | or    | 11 km | 02 km | 87 km | 35 km |
|       |       | 36″   | vi    |       |       |       |       |
|       |       |       | llage |       |       |       |       |
+-------+-------+-------+-------+-------+-------+-------+-------+
| 3     | 0.001 | 0°    | ne    | 111 m | 102 m | 7     | 4     |
|       |       | 00′   | i     |       |       | 8.7 m | 3.5 m |
|       |       | 3.6″  | ghbor |       |       |       |       |
|       |       |       | hood, |       |       |       |       |
|       |       |       | s     |       |       |       |       |
|       |       |       | treet |       |       |       |       |
+-------+-------+-------+-------+-------+-------+-------+-------+
| 4     | 0     | 0°    | indiv | 1     | 1     | 7     | 4     |
|       | .0001 | 00′   | idual | 1.1 m | 0.2 m | .87 m | .35 m |
|       |       | 0.36″ | st    |       |       |       |       |
|       |       |       | reet, |       |       |       |       |
|       |       |       | large |       |       |       |       |
|       |       |       | buil  |       |       |       |       |
|       |       |       | dings |       |       |       |       |
+-------+-------+-------+-------+-------+-------+-------+-------+
| 5     | 0.    | 0°    | indiv | 1     | 1     | 0.    | 0.    |
|       | 00001 | 00′ 0 | idual | .11 m | .02 m | 787 m | 435 m |
|       |       | .036″ | t     |       |       |       |       |
|       |       |       | rees, |       |       |       |       |
|       |       |       | h     |       |       |       |       |
|       |       |       | ouses |       |       |       |       |
+-------+-------+-------+-------+-------+-------+-------+-------+
| 6     | 0.0   | 0°    | indiv | 1     | 1     | 78    | 43    |
|       | 00001 | 00′   | idual | 11 mm | 02 mm | .7 mm | .5 mm |
|       |       | 0.    | h     |       |       |       |       |
|       |       | 0036″ | umans |       |       |       |       |
+-------+-------+-------+-------+-------+-------+-------+-------+
| 7     | 0.00  | 0°    | prac  | 11    | 10    | 7.    | 4.    |
|       | 00001 | 00′   | tical | .1 mm | .2 mm | 87 mm | 35 mm |
|       |       | 0.0   | limit |       |       |       |       |
|       |       | 0036″ | of    |       |       |       |       |
|       |       |       | comme |       |       |       |       |
|       |       |       | rcial |       |       |       |       |
|       |       |       | surv  |       |       |       |       |
|       |       |       | eying |       |       |       |       |
+-------+-------+-------+-------+-------+-------+-------+-------+
| 8     | 0.000 | 0°    | s     | 1.    | 1.    | 0.7   | 0.4   |
|       | 00001 | 00′   | pecia | 11 mm | 02 mm | 87 mm | 35 mm |
|       |       | 0.00  | lized |       |       |       |       |
|       |       | 0036″ | surv  |       |       |       |       |
|       |       |       | eying |       |       |       |       |
|       |       |       | (e.   |       |       |       |       |
|       |       |       | g     |       |       |       |       |
|       |       |       | . tec |       |       |       |       |
|       |       |       | tonic |       |       |       |       |
|       |       |       | pla   |       |       |       |       |
|       |       |       | t     |       |       |       |       |
|       |       |       | e map |       |       |       |       |
|       |       |       | ping) |       |       |       |       |
+-------+-------+-------+-------+-------+-------+-------+-------+

```{r include=FALSE, decimal_points_precision_data}

decimal_points_data <- readxl::read_excel("Decimal_points.xlsx")

knitr::kable(decimal_points_data)


```

Number of decimal points in long. and lat. values impact the precision
of data and its ability to unambiguously identify points of interest,
and according to the table above values with 4 decimal points can
unambiguously identify an individual street and large building. Three
decimal points are used for the unambiguous identification of
neighborhoods and streets. Identification with values that are composed
of less than 3 decimal points is used for identification of towns, large
cities, and countries which does not hold any relevance for
navigator.ba. Data analysis shows 29 537 (56%) long. values have 4 or
more than 4 digits and, while 66 (\< 1%) have 3 decimal points. Finally,
38 data have less than three decimal points and can be considered as
data of low quality. The precision of lat. values are less affected by
the number of decimal points, hence data related to the number of
decimal points presented below are limited only to long. values.

```{r}
countDecimals<-function(x){
  stopifnot(class(x)=="character")
  x<-gsub("(.*)(\\.)|([0]*$)","",x)
  nchar(x)
}

business_longitude_decimal_points_graph <- business_longitude_decimal_points <- data %>% 
                                    dplyr::mutate(
                                      business_longitude = as.character(business_longitude),
                                      num_of_decimals_long_values = countDecimals(business_longitude)) %>%
                                    dplyr::select(business_longitude, num_of_decimals_long_values) %>%
                                    dplyr::mutate(num_of_decimals_in_longitudinal_values = as.factor(num_of_decimals_long_values),
                                          total = 1) %>%
                                    dplyr::group_by(num_of_decimals_in_longitudinal_values)%>%
                                    dplyr::summarise(n = sum(total))%>%
                                    ggplot(aes(num_of_decimals_in_longitudinal_values, n))+
                                    geom_col(fill = rgb(0,51,160, maxColorValue = 255))+
                                    geom_text(aes(label=n), vjust=-0.2,size=4, fontface="bold")+
                                    theme_classic()+
                                    xlab("Number of decimals")+
  labs(title="Number of decimal points in long.\nvalues")+
                                    theme(
                                      axis.title.y = element_blank(),
                                      plot.title = element_text(margin=margin(0,0,20,0), size =18, face="bold")
                                    )

business_longitude_decimal_points_graph                       
```

To analyze the precision of available data I have conducted row wise
analysis of long. and lat. values. The analysis included a group
summation of each level of decimal points available in the data set. I
converted the data type in the business longitude column from character
to factor. After the conversion of the data, type was completed I
grouped and summarized the data based on each available factor level to
get the total number of longs. values per number of decimal points. The
table below shows the number of decimal points in all examined data.
There are 22 674 (43%) missing long. and lat. values in the dataset out
of which 9 rows are also missing data in the column business name.

```{r include=FALSE, decimal_points_analysis}

business_longitude_decimal_points <- data %>% 
                                   dplyr::mutate(business_longitude = as.character(business_longitude),
                                     num_of_decimals_long_values = countDecimals(business_longitude)) %>%
                                    dplyr::select(business_longitude, num_of_decimals_long_values) %>%
                                    dplyr::mutate(num_of_decimals_in_longitudinal_values = as.factor(num_of_decimals_long_values),
                                         total = 1) %>%
                                    dplyr::group_by(num_of_decimals_in_longitudinal_values) %>%
                                    dplyr::summarise(n = sum(total))

names(business_longitude_decimal_points)[1] <- "Number of decimal points"

                                    
knitr::kable(business_longitude_decimal_points)
```

## Data standardization

I also conducted an analysis of the standards used in the data set as
they are critical when it comes to processing and mapping data. For the
business city, data analysis shows that there are inconsistencies in
entering the name of San Francisco. Data below show that there are 52
148 values "San Francisco", 136 SF, and 31 values are missing.

|     City      | Total |
|:-------------:|:-----:|
|      NA       |  31   |
| San Francisco | 52148 |
|      SF       |  136  |

Analysis of state name data show that there 52 089 values entered as CA
and 97 as California. Also, the analysis revealed that there are 129
data entered as ISO country code for Illinois which are incorrect data
as San Francisco is in the state of California, USA.

|   State    | Total |
|:----------:|:-----:|
|     CA     | 52089 |
| California |  97   |
|     IL     |  129  |

```{r include=FALSE}
data$business_city <- as.factor(data$business_city)

summarized_business_city_data <- data %>% 
                                 dplyr::mutate(n = 1) %>%
                                 dplyr::group_by(business_city) %>%
                                 dplyr::summarise(total = sum(n))

summarized_business_city_data <- dplyr::mutate_if(summarized_business_city_data, is.factor, as.character)
summarized_business_city_data[1,1] <-  "NA"
names(summarized_business_city_data)[1] <-  "Business city"
names(summarized_business_city_data)[2] <-  "Total"

```

```{r include=FALSE}
data$business_state <- as.factor(data$business_state)

summarized_business_state_data <- data %>% 
                                 dplyr::mutate(n = 1) %>%
                                 dplyr::group_by(business_state) %>%
                                 dplyr::summarise(total = sum(n))

summarized_business_state_data  <- dplyr::mutate_if(summarized_business_state_data , is.factor, as.character)
#summarized_business_city_data[1,1] <-  "NA"
names(summarized_business_state_data)[1] <-  "Business state"
names(summarized_business_state_data)[2] <-  "Total"

```

```{r include=FALSE, calculations}

summary(data)

names(data)
#check if lat. values are in range between -+180
validate_latitude <- data %>% dplyr::filter(business_latitude > 90 | business_latitude < -90)
nrow(validate_latitude)

#check if long. values are in range between -+90
validate_longitude <- data %>% dplyr::filter(business_longitude> 180 | business_longitude < -180)
nrow(validate_longitude)

####################
# 4 values are not in the range valid range for long and lat values
####################


#check number of unique values for business name, lat. and long
length(unique(data$business_name))
length(unique(data$business_latitude))
length(unique(data$business_longitude))

##################
## discrepancu between number of distinct values for address long and latitude value
##################
sort(unique(data$business_name))

empty_and_unknown_values <- data %>% dplyr::filter(business_name != '', business_name !='hidden', business_name !='Hidden', business_name != 'Unavailable')

nrow(empty_and_unknown_values)

sort(empty_and_unknown_values$business_name)
data %>% dplyr::filter(stringr::str_detect(business_name, "^HIDD"), business_name != '')

################
# unknown, hidden data etc, were entered inconsistently i.e sometimes with capital letter and on other ocassions with lower letter
###################


# MISSING VALUES

total_missing_business_names <-  (nrow(data) - length(na.omit(data$business_name)))+(nrow(data) -nrow(empty_and_unknown_values))


missing_long_values <- data %>% 
                        dplyr::filter(business_name != '', business_name !='hidden', business_name !='Hidden', business_name != 'Unavailable') %>%
                        dplyr::select(business_longitude)%>%
                        dplyr::mutate(long_as_string = as.character(business_longitude))%>%
                        dplyr::filter(long_as_string == '')


missing_lat_values <- data %>% 
                        dplyr::filter(business_name != '', business_name !='hidden', business_name !='Hidden', business_name != 'Unavailable') %>%
                        dplyr::select(business_latitude)%>%
                        dplyr::mutate(long_as_string = as.character(business_latitude))%>%
                        dplyr::filter(long_as_string == '')



## Missing all critical values

bussines_names_long_lat_values_validation <- data %>% dplyr::mutate_all(~ ifelse(is.na(.x),"NA",.x))
bussines_names_long_lat_values_validation$business_name <-  stringr::str_replace(bussines_names_long_lat_values_validation$business_name,"hidden","NA")
bussines_names_long_lat_values_validation$business_name <-  stringr::str_replace(bussines_names_long_lat_values_validation$business_name,"Hidden","NA")
bussines_names_long_lat_values_validation$business_name <-  stringr::str_replace(bussines_names_long_lat_values_validation$business_name,"Unavailable","NA")

nrow(bussines_names_long_lat_values_validation %>% dplyr::filter(business_name == 'NA', business_latitude == 'NA', business_longitude == 'NA'))


# PRECISION OF LONG AND LAT VALUES (number of decimal values)

## create function to calculate number of decimal points

business_longitude_decimal_points <- data %>% 
                                    dplyr::mutate(
                                      business_longitude = as.character(business_longitude),
                                      num_of_decimals_long_values = countDecimals(business_longitude)) %>%
                                    dplyr::select(business_longitude, num_of_decimals_long_values) %>%
                                    dplyr::mutate(num_of_decimals_in_longitudinal_values = as.factor(num_of_decimals_long_values),
                                          total = 1) %>%
                                    dplyr::group_by(num_of_decimals_in_longitudinal_values) %>%
                                    dplyr::summarise(n = sum(total))%>%
                                    dplyr::mutate(num_of_decimals_in_longitudinal_values = as.numeric(num_of_decimals_in_longitudinal_values)) %>%
                                    dplyr::filter(num_of_decimals_in_longitudinal_values < 3) %>%
                                    dplyr::summarise(total = sum(n))
                                    

                                    
business_latitude_decimal_points <- data %>% 
                                    dplyr::mutate(
                                      business_latitude = as.character(business_latitude),
                                      num_of_decimals_lat_values = countDecimals(business_latitude)) %>%
                                    dplyr::select(business_latitude, num_of_decimals_lat_values) %>%
                                    dplyr::mutate(num_of_decimals_in_latitude_values = as.factor(num_of_decimals_lat_values),
                                          total = 1) %>%
                                    dplyr::group_by(num_of_decimals_in_latitude_values) %>%
                                    dplyr::summarise(n = sum(total)) %>%
                                    dplyr::mutate(num_of_decimals_in_latitude_values = as.numeric(num_of_decimals_in_latitude_values)) %>%
                                    dplyr::filter(num_of_decimals_in_latitude_values == 3) %>%
                                    dplyr::summarise(total = sum(n))





```

## Data verification

The sample was selected randomly from rows with complete critical data
(i.e. business name, longitude, and latitude). Duplicate values for
business names were excluded from the dataset prior to sampling. Data
were shuffled to get random values from the dataset. Additionally, from
column business location which includes full business location data
(i.e. long. and lat. values), brackets were removed to enable me to
fetch data from an external API. I planned to take 10% of the total data
for the data verification but due to the technical issues with API call,
I managed to sample 497 data (9% of total data with unique values)

Sample data were exported in Excel worksheet and then imported into
Google sheets. API connector add-in, available in Google sheets was used
to make an API request to position stack API. Positionstack has the
option of sending batch API call by referencing column with full
location data (long. and lat. values separated by comma). API URL to
which GET request was sent is
[\<http://api.positionstack.com/v1/forwar>](http://api.positionstack.com/v1/forward)?
access_key = \<USER ACCESS KEY> &
query=+++Master_sheet!I2:I498+++&limit=1. All data from Excel file were
copied to Master_sheet and API results were stored in ValidationSheet.
Returned API data shows that out of **497 data sampled from the original
dataset, there are 39 (8%) values in** **business name column with
identical long. and lat. values.** Based on the findings from sampled
data I engaged in the analysis of duplicate values for a totalof 5 423
unique business names. **Data shows that 370 unique business names (7%)
share identical long. and lat. values.**

Analysis of sample data was also done in an Excel file (data.xlsx). Data
retrieved from API call in Google sheet were copied in sheet
validatedData and then using VLOOKUP in Excel file returned addresses
were compared to addresses available in Master sheet to check if long.
and lat. values are the same in both sheets. **Analysis of data showed
that out of 497 data, 474 (95%) addresses returned from API did not
match with addresses available from the original data set from the
supplier.** Considering that almost 95 percent of the data did not match
I engaged in the manual analysis of the first 15 entries used in the API
call to check their accuracy on Google maps and the data returned from
API were consistent with data returned by Google maps.

```{r include=FALSE, sample_check}

sample_data <- data %>%
              dplyr::filter(business_name != "", business_location != "")

# get information about the number of unique business names values that have complete set of critical data -
# business name & business location (long and lat value)
sample_data <- sample_data [!duplicated(sample_data$business_name),]

length(sample_data$business_name)

number_of_data_without_duplicate_names <- nrow(sample_data)

sample_data <- sample_data [!duplicated(sample_data$business_location),]

number_of_data_without_duplicate_long_and_lat_values <- nrow(sample_data)

total_number_of_locations_with_duplicate_long_and_lat_values = number_of_data_without_duplicate_names - number_of_data_without_duplicate_long_and_lat_values


set.seed(42)
rows <- sample(nrow(sample_data))

sample_data_for_validation <- sample_data[rows,]
sample_data_for_validation <- sample_data_for_validation[1:500,]
sample_data_for_validation$business_location <- stringr::str_remove(sample_data_for_validation$business_location,"\\(")
sample_data_for_validation$business_location <- stringr::str_remove(sample_data_for_validation$business_location,"\\)")


wb <-  openxlsx::createWorkbook()
openxlsx::addWorksheet(wb, sheetName = 'data',gridLines = TRUE)
openxlsx::writeData(wb, sheet = "data", x = sample_data_for_validation, rowNames = FALSE)
openxlsx::saveWorkbook(wb, "/Users/adnanovcina/data_analysis_assignment/data.xlsx", overwrite = TRUE)

```

# Final remarks and recommendations

Analysis of data as presented above identified many errors in terms of
the number of missing values, data completeness, and data accuracy.
Considering the findings from the analysis of a random sample collected
from the original data set conclusion can be drawn that all data
available in the data set are of questionable quality and require
extensive cleaning before they can be used in navigator.ba app for
displaying data to end-user.

The recommendation is not to engage in further negotiation with the
supplier. Efforts should be made to identify another source of POIs.

```{r}
r_
```
